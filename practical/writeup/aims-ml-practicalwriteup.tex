\documentclass[10pt]{article}

\usepackage{amsmath, amsthm, amsfonts}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{todonotes}

\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}

\lstset{
basicstyle=\ttfamily\footnotesize,
columns=fixed,
fontadjust=true,
basewidth=0.5em,
frame=single
}

\newcommand{\EE}{\mathbb{E}}
\newcommand{\bigoh}{\mathcal{O}}

\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\adv}{adv}

\title{AIMS Machine Learning Course 2017 \\ \emph{Solutions to Practical 3}}
\author{Chris Nicholls}
\begin{document}

\maketitle

\section{Overview of algorithms}

\subsection{Thompson sampling}
I implemented Thompson sampling using a beta distribution. I used the prior
$\Beta(1,1)$ and then updated each arm when I received a reward for it. After
observing $\alpha$ 1s and $n - \alpha$ 0s from arm $a$, the posterior is
$\Beta(\alpha + 1, n - \alpha + 1)$.

Given the posterior for each arm $a$, we sample a mean $\tilde{\mu}_a$, and then pick
the arm $\tilde{a} = \argmax_a \tilde{\mu}_a$.

The results using Thompson sampling are displayed in Figure~\ref{figure-graphs}.

\section{Task 1}

We model the reward distribution of each arm $a$ as a Bernoulli distribution
with mean $\mu_a$. We considered two methods of selecting $\mu_a$. Firstly, we
consider sampling $\mu_a$ for each arm uniformly in $[0,1]$. We call this the
\emph{randomly chosen means setup}. Secondly, we consider the lower bound
construction as in the lectures. With $K$ arms, we choose uniformly at random
among the $K$ setups $I_i$, where
\begin{align*}
    \mu_j =
    \begin{cases}
        \frac{1}{2} + \epsilon, & \textrm{if} \hspace{0.2cm} j = i \\
        \frac{1}{2}, & \textrm{if} \hspace{0.2cm} j \not= i.
    \end{cases}
\end{align*}
We call this the \emph{lower bound setup}.

Thus in the lower bound setup, we randomly choose one of the arms to have reward
$1/2 + \epsilon$, and all other arms have reward $1/2$. It is shown in lectures
that all bandit algorithms have expected cumulative regret $\Omega(\sqrt{K T})$
for this setup. In the below, I set $\epsilon = 0.1$. Feasibly I could have set
this smaller, making it harder for the algorithms, but I didn't have enough
computing power to take $T$ large enough for this to be visible.

I implemented uniform exploration, epsilon-greedy, successive elimination, UCB1,
Thompson sampling, and Exp4 (the generalisation of Hedge), and tried each
algorithm out on both bandit setups. The following are the results. As suggested
in Task 1, I fixed $K = 10$ arms and focused on the dependence on $T$. Each is a
plot of $\log R(T)$ against $\log T$, where $R(T)$ denotes the expected
cumulative regret. To approximate $R(T)$, I ran each experiment 20 times and
averaged the results.

The algorithms have the following bounds on expected cumulative regrets:
\begin{itemize}
    \item Uniform exploration: with $N$ chosen as $\bigoh( (T/K)^{2/3} (\log
    T)^{1/3} )$ we have $\EE(R(T)) \le \bigoh(T^{2/3} (K \log T)^{1/3})$.
    \item Epsilon-greedy: with exploration probabilities $\epsilon_t = t^{-1/3}
    (K \log t)^{1/3}$ achieves regret bound $\EE(R(T)) \le \bigoh( t^{2/3} (K
    \log t)^{1/3} )$.
    \item Successive elimination: achieves regret bound $\EE(R(t)) =
    \bigoh(\sqrt{K t \log T})$ for all rounds $t \le T$.
    \item UCB1: achieves regret bound $\EE(R(t)) = \bigoh(\sqrt{K t \log T})$
    for all rounds $t \le T$.
    \item Thompson sampling: the regret is bounded by $R(T) = \bigoh(KT \log T)$
    (\cite{Slivkins}, Theorem 4.12).
    \item Exp4: with parameters $\gamma \in [0, \frac{1}{2T})$ and $\epsilon =
    \sqrt{\log K / (3 U)}$ where $U = K / (1-\gamma)$, the regret is bounded by
    $\EE (R(T)) \le 2 \sqrt{3} \sqrt{T K \log K} + 1$. \todo{We have to pick
    epsilon correctly!}
\end{itemize}

Thus $\log \EE( R(T) ) \le \frac{2}{3} \log T + \bigoh(\log (K \log T))$ for
uniform exploration and epsilon-greedy. For successive elimination and UCB1, the
slope of $\log \EE( R(T))$ against $\log T$ is bounded by $\frac{1}{2}$. We
analyse this in the following graphs.

We expect that for the uniformly random chosen means, the slopes are at most
$2/3, 2/3, 1/2, 1/2$, respectively. For the lower bound means, we are expecting
that the slopes should all be at least $1/2$. In Figure~\ref{figure-graphs}, the
top two graphs show the results for the lower bound means (top left) and for the
randomly chosen means (top right).

Interestingly, for the lower bound construction, the gradient of all the lines
are very similar, roughly equal to 1. Each bandit algorithm we implemented has a
regret bound of at most $T^{2/3}$, and some of them have $T^{1/2}$, so this is
surprising. However, we can't actually conclude much from such a diagram, as
this regret bound is only guaranteed asymptotically.

%\begin{figure}[!ht]
%    \center
%    \begin{tabular}{cc}
%        \includegraphics[width=7cm]{"plots/uniform-random"} &
%        \includegraphics[width=7cm]{"plots/epsilongreedy-random"} \\
%        \includegraphics[width=7cm]{"plots/successiveelimination-random"} &
%        \includegraphics[width=7cm]{"plots/ucb1-random"} \\
%        \includegraphics[width=7cm]{"plots/thompson-random"} & 
%    \end{tabular}
%    \caption{Uniformly random chosen means}
%    \label{figure-random}
%\end{figure}
%
%\begin{figure}[!ht]
%    \center
%    \begin{tabular}{cc}
%        \includegraphics[width=7cm]{"plots/uniform-lower"} &
%        \includegraphics[width=7cm]{"plots/epsilongreedy-lower"} \\
%        \includegraphics[width=7cm]{"plots/successiveelimination-lower"} &
%        \includegraphics[width=7cm]{"plots/ucb1-lower"} \\
%        \includegraphics[width=7cm]{"plots/thompson-lower"} & 
%    \end{tabular}
%    \caption{Means from lower bound construction}
%    \label{figure-lower}
%\end{figure}

\section{Task 2}

\subsection{Adversarial bandits}

For the adversarial bandit setting, I investigated the following scenario,
suggested in the problem description. I sampled two separate mean vectors
$\mu^1, \mu^2$ uniformly in $[0,1]^K$. I also chose a transition probability $p$
and then give the arms the Bernoulii distribution with mean vector $\mu$, where
$\mu$ is chosen as follows.  Initially, $\mu = \mu^1$. On each round, with
probability $p$ the mean vector $\mu$ is selected as the other mean vector.



Note that we define the reward in hindsight as the sum of the mean rewards of
all the arms (since we privately know all the means).

We use the adversarial bandit algorithm called \verb'Exp4' from \cite{Slivkins},
shown in Figure~\ref{figure-algo-exp4}

\begin{figure}[!ht]
\begin{lstlisting}
Given a set E of experts, and parameters eps, gamma in (0,1/2)
Initialise weight w_1(a) = 1 for all arms a
For each round t:
    Compute p_t(a) = w_t(a) / sum_{a'} w_t(a')

    Draw expert e_t from p_t
    Get recommendation of action b_t from expert e_t
    With probability gamma:
        pick action a_t uniformly at random
    Else: 
        pick action a_t = b_t
    Observe cost c_t(a_t) for playing action a_t
    Define fake costs for all experts:
        fc_t(a) = c_t(a) / p_t(a), if a = b_t
                = 0, else

    Update weights w_{t+1}(a) = w_t(a) * (1-eps)^{fc_t(a)}
\end{lstlisting}
\caption{The algorithm `Exp4' from \cite{Slivkins}}
\label{figure-algo-exp4}
\end{figure}



Let $R(T)$ denote the cumulative regret of an algorithm, run up to time $T$. We
plot $\log R(T)$ against $\log T$ for all the algorithms.


\begin{remark}
    It seems to affect the variance of our sample of $\EE(R(T))$ a lot whether
    you use $R(T) = T \max_a \mu(a) - \sum_{t=1}^T r_t(a_t)$ or, the correct
    definition, $R(T) = T \max_a \mu(a) - \sum_{t=1}^T \mu(a_t)$. In the first,
    you use the actual reward received by the algorithm, and in the second, we
    use the mean reward!
\end{remark}



We have two types of regret. In the standard setting (i.e. non adversarial), we
use the regret
\begin{align*}
    R(T) = \max_a \mu(a) - \sum_{t=1}^T \mu(a_t),
\end{align*}
where $a_1, \ldots, a_T$ is the sequence of actions the algorithm makes, and
$\mu_1, \ldots, \mu_K$ is the mean reward vector. Note that the second term
differs from $\sum_{t=1}^T r_t(a_t)$, which is the reward the algorithm
actually receives. In fact, the algorithm cannot compute $R(T)$ without knowing
the mean vector $\mu$. However, we can compute this in practice since we
generate $\mu$ in the program.

In the adversarial setting, we use the hindsight regret. We assume an oblivious
adversary, and so can assume that the rewards $r_t(a)$ are generated beforehand
for all $t = 1, \ldots, T$ and arms $a$. We call this the \emph{reward table}.
The reward in hindsight is then
\begin{align*}
    R_\adv(T) = \max_a \sum_{t=1}^T r_t(a) - \sum_{t=1}^T r_t(a_t),
\end{align*}
where $a_1, \ldots, a_T$ is the sequence of actions the algorithm makes, and
$r_t(a_t)$ is the sequence of rewards the algorithm receives. Thus the hindsight
regret is the difference between the reward of the best arm in hindsight minus
the reward the algorithm actually received.

\begin{remark}
    We could feasibly use 
\end{remark}

\begin{figure}[!ht]
    \center
    \begin{tabular}{cc}
        \includegraphics[width=7cm]{"plots/standard-all-lower"} &
        \includegraphics[width=7cm]{"plots/standard-all-random"} \\
        \includegraphics[width=7cm]{"plots/adversarial-all-lower"} &
        \includegraphics[width=7cm]{"plots/adversarial-all-random"} \\
    \end{tabular}
    \caption{Log cumulative regret vs $\log T$ for the various bandit scenarios.
    Top left: standard setup with lower bound means; top right: standard setup
    with randomly chosen means; bottom left: adversarial setup with lower bound
    means; bottom right: adversarial setup with randomly chosen means.}
    \label{figure-graphs}
\end{figure}

\bibliographystyle{alpha}
\bibliography{aims-ml-practicalwriteup}

\end{document}
